{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using GeForce RTX 3080 GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == torch.device('cpu'):\n",
    "    print('Using cpu')\n",
    "else:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "source": [
    "# Cleaning the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic\n0       Explanation\\nWhy the edits made under my usern...      0\n1       D'aww! He matches this background colour I'm s...      0\n2       Hey man, I'm really not trying to edit war. It...      0\n3       \"\\nMore\\nI can't make any real suggestions on ...      0\n4       You, sir, are my hero. Any chance you remember...      0\n...                                                   ...    ...\n159566  \":::::And for the second time of asking, when ...      0\n159567  You should be ashamed of yourself \\n\\nThat is ...      0\n159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n159569  And it looks like it was actually you who put ...      0\n159570  \"\\nAnd ... I really don't think you understand...      0\n\n[159571 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_open = pd.read_csv('C:/Users/OB/Desktop/projects_to_do/project_7/toxic_comments.csv')\n",
    "display(df_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "def remove_numbers(text):\n",
    "    return text.translate(str.maketrans('', '', digits))\n",
    "def known_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contraction_mapping:\n",
    "            text = text.replace(word, contraction_mapping[word.lower()])\n",
    "    return text\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic\n0       explanation edits made username hardcore metal...      0\n1       daww matches background colour seemingly stuck...      0\n2       hey man really trying edit war guy constantly ...      0\n3       cannot make real suggestions improvement wonde...      0\n4                           sir hero chance remember page      0\n...                                                   ...    ...\n159566  second time asking view completely contradicts...      0\n159567               ashamed horrible thing put talk page      0\n159568  spitzer umm theres actual article prostitution...      0\n159569  looks like actually put speedy first version d...      0\n159570  really think understand came idea bad right aw...      0\n\n[159571 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>explanation edits made username hardcore metal...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>daww matches background colour seemingly stuck...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hey man really trying edit war guy constantly ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cannot make real suggestions improvement wonde...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sir hero chance remember page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>second time asking view completely contradicts...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ashamed horrible thing put talk page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>spitzer umm theres actual article prostitution...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>looks like actually put speedy first version d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>really think understand came idea bad right aw...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df_open.copy()\n",
    "df['text']=df['text'].astype(str) #fix the format\n",
    "df['text']=df['text'].str.lower() #lower the strings\n",
    "df['text']=df['text'].apply(lambda text: remove_numbers(text)) #delete numbers\n",
    "df['text']=df['text'].apply(lambda text: known_contractions(text)) #check the grammar\n",
    "df['text']=df['text'].replace('https?:\\/\\/.*\\/\\w*', '', regex=True) #delete hyperlinks\n",
    "df['text']=df['text'].replace('#', ' ', regex=True) #delete hashtags\n",
    "df['text']=df['text'].replace('\\@\\w*', '', regex=True) #delete quotes\n",
    "df['text']=df['text'].replace('\\$\\w*', '', regex=True) #delete tickers\n",
    "df['text']=df['text'].apply(lambda text: remove_punctuation(text)) #delete punktuation\n",
    "df['text']=df['text'].apply(lambda text: remove_stopwords(text)) #delete stopwords\n",
    "df['text']=df['text'].replace('\\&*[amp]*\\;|gt+', '', regex=True) #delete quotes\n",
    "df['text']=df['text'].replace('\\s+rt\\s+', '', regex=True) #delete RT\n",
    "df['text']=df['text'].replace('[\\n\\t\\r]+', ' ', regex=True) #delete linebreak, tab, return\n",
    "df['text']=df['text'].replace('via+\\s', '', regex=True) #delete via\n",
    "df['text']=df['text'].replace('\\s+\\s+', ' ', regex=True) #убираем hyperspaces\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                     text  toxic  \\\n0       explanation edits made username hardcore metal...      0   \n1       daww matches background colour seemingly stuck...      0   \n2       hey man really trying edit war guy constantly ...      0   \n3       cannot make real suggestions improvement wonde...      0   \n4                           sir hero chance remember page      0   \n...                                                   ...    ...   \n159566  second time asking view completely contradicts...      0   \n159567               ashamed horrible thing put talk page      0   \n159568  spitzer umm theres actual article prostitution...      0   \n159569  looks like actually put speedy first version d...      0   \n159570  really think understand came idea bad right aw...      0   \n\n                                               text_stems  \\\n0       explan edit made usernam hardcor metallica fan...   \n1       daww match background colour seem stuck thank ...   \n2       hey man realli tri edit war guy constant remov...   \n3       cannot make real suggest improv wonder section...   \n4                              sir hero chanc rememb page   \n...                                                   ...   \n159566  second time ask view complet contradict covera...   \n159567                  asham horribl thing put talk page   \n159568  spitzer umm there actual articl prostitut ring...   \n159569  look like actual put speedi first version dele...   \n159570  realli think understand came idea bad right aw...   \n\n                                              text_lemmas  \n0       explanation edits make username hardcore metal...  \n1       daww match background colour seemingly stuck t...  \n2       hey man really try edit war guy constantly rem...  \n3       cannot make real suggestion improvement wonder...  \n4                           sir hero chance remember page  \n...                                                   ...  \n159566  second time ask view completely contradict cov...  \n159567               ashamed horrible thing put talk page  \n159568  spitzer umm there actual article prostitution ...  \n159569  look like actually put speedy first version de...  \n159570  really think understand come idea bad right aw...  \n\n[159571 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>toxic</th>\n      <th>text_stems</th>\n      <th>text_lemmas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>explanation edits made username hardcore metal...</td>\n      <td>0</td>\n      <td>explan edit made usernam hardcor metallica fan...</td>\n      <td>explanation edits make username hardcore metal...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>daww matches background colour seemingly stuck...</td>\n      <td>0</td>\n      <td>daww match background colour seem stuck thank ...</td>\n      <td>daww match background colour seemingly stuck t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hey man really trying edit war guy constantly ...</td>\n      <td>0</td>\n      <td>hey man realli tri edit war guy constant remov...</td>\n      <td>hey man really try edit war guy constantly rem...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cannot make real suggestions improvement wonde...</td>\n      <td>0</td>\n      <td>cannot make real suggest improv wonder section...</td>\n      <td>cannot make real suggestion improvement wonder...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sir hero chance remember page</td>\n      <td>0</td>\n      <td>sir hero chanc rememb page</td>\n      <td>sir hero chance remember page</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>second time asking view completely contradicts...</td>\n      <td>0</td>\n      <td>second time ask view complet contradict covera...</td>\n      <td>second time ask view completely contradict cov...</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ashamed horrible thing put talk page</td>\n      <td>0</td>\n      <td>asham horribl thing put talk page</td>\n      <td>ashamed horrible thing put talk page</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>spitzer umm theres actual article prostitution...</td>\n      <td>0</td>\n      <td>spitzer umm there actual articl prostitut ring...</td>\n      <td>spitzer umm there actual article prostitution ...</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>looks like actually put speedy first version d...</td>\n      <td>0</td>\n      <td>look like actual put speedi first version dele...</td>\n      <td>look like actually put speedy first version de...</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>really think understand came idea bad right aw...</td>\n      <td>0</td>\n      <td>realli think understand came idea bad right aw...</td>\n      <td>really think understand come idea bad right aw...</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 4 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_stems'] = df['text'].apply(stem_words)\n",
    "df['text_lemmas'] = df['text'].apply(lemmatize_words)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['text_lemmas']\n",
    "target = df['toxic']\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=123455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Размер матрицы: (127656, 181158)\nРазмер матрицы: (31915, 181158)\nРазмер матрицы: (127656,)\nРазмер матрицы: (31915,)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_train_feautures = count_tf_idf.fit_transform(features_train)\n",
    "tf_idf_test_feautures = count_tf_idf.transform(features_test)\n",
    "#print(tf_idf_train_feautures)\n",
    "#print(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", tf_idf_train_feautures.shape)\n",
    "print(\"Matrix shape:\", tf_idf_test_feautures.shape)\n",
    "print(\"Matrix shape:\", target_train.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General TF-IDF + ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (31915,)\nMatrix shape: (31915,)\nf1 log: 0.7485461091110496\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345,solver='liblinear',class_weight='balanced')\n",
    "model.fit(tf_idf_train_feautures, target_train)\n",
    "predictions = model.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1 = f1_score(target_test, predictions)\n",
    "print(\"f1 log:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SGDClassifier(penalty='none', random_state=12345)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "modelsgd = SGDClassifier(max_iter=1000,shuffle=True,random_state=12345)\n",
    "grid_search_sgd = GridSearchCV(modelsgd, param_grid=params)\n",
    "grid_search_sgd.fit(tf_idf_train_feautures, target_train)\n",
    "print(grid_search_sgd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Размер матрицы: (31915,)\nРазмер матрицы: (31915,)\nf1 sgd: 0.7732629727352682\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
    "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='none',\n",
    "              power_t=0.5, random_state=12345, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "sgd.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_sgd = sgd.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_sgd.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_sgd = f1_score(target_test, predictions_sgd)\n",
    "print(\"f1 sgd:\",f1_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostClassifier()\n",
    "model_cb_index = [0,1,2,3,4,5,6]\n",
    "params_dist_cb = {'iterations': [500],\n",
    "          'learning_rate':[0.01,0.05,0.1],\n",
    "          'loss_function':['Logloss', 'CrossEntropy'],\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\n",
    "          'leaf_estimation_iterations': [10],\n",
    "           'eval_metric': ['F1'],\n",
    "           'use_best_model': ['True'],\n",
    "          'logging_level':['Silent'],\n",
    "          'random_seed': [123455]\n",
    "         }\n",
    "grid_search_cb = GridSearchCV(model_cb, params_dist_cb, scoring=make_scorer(f1_score))\n",
    "grid_search_cb.fit(tf_idf_train_feautures, target_train)\n",
    "print(grid_search_cb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6824394\ttotal: 5.7s\tremaining: 47m 23s\n",
      "1:\tlearn: 0.6714687\ttotal: 10.7s\tremaining: 44m 23s\n",
      "2:\tlearn: 0.6611103\ttotal: 15.6s\tremaining: 43m 3s\n",
      "3:\tlearn: 0.6514688\ttotal: 20.5s\tremaining: 42m 20s\n",
      "4:\tlearn: 0.6419060\ttotal: 25.4s\tremaining: 41m 53s\n",
      "5:\tlearn: 0.6324299\ttotal: 30.3s\tremaining: 41m 33s\n",
      "6:\tlearn: 0.6231531\ttotal: 35.2s\tremaining: 41m 17s\n",
      "7:\tlearn: 0.6137622\ttotal: 40.3s\tremaining: 41m 17s\n",
      "8:\tlearn: 0.6045535\ttotal: 45.2s\tremaining: 41m 5s\n",
      "9:\tlearn: 0.5955023\ttotal: 50.1s\tremaining: 40m 54s\n",
      "10:\tlearn: 0.5870111\ttotal: 55.1s\tremaining: 40m 48s\n",
      "11:\tlearn: 0.5785621\ttotal: 60s\tremaining: 40m 39s\n",
      "12:\tlearn: 0.5700064\ttotal: 1m 4s\tremaining: 40m 34s\n",
      "13:\tlearn: 0.5620340\ttotal: 1m 9s\tremaining: 40m 25s\n",
      "14:\tlearn: 0.5542361\ttotal: 1m 14s\tremaining: 40m 20s\n",
      "15:\tlearn: 0.5467100\ttotal: 1m 19s\tremaining: 40m 16s\n",
      "16:\tlearn: 0.5391806\ttotal: 1m 24s\tremaining: 40m 11s\n",
      "17:\tlearn: 0.5319983\ttotal: 1m 29s\tremaining: 40m 4s\n",
      "18:\tlearn: 0.5246560\ttotal: 1m 34s\tremaining: 40m 2s\n",
      "19:\tlearn: 0.5178252\ttotal: 1m 40s\tremaining: 40m 1s\n",
      "20:\tlearn: 0.5109990\ttotal: 1m 45s\tremaining: 40m 3s\n",
      "21:\tlearn: 0.5045136\ttotal: 1m 50s\tremaining: 40m\n",
      "22:\tlearn: 0.4980655\ttotal: 1m 55s\tremaining: 39m 56s\n",
      "23:\tlearn: 0.4918424\ttotal: 2m\tremaining: 39m 47s\n",
      "24:\tlearn: 0.4857251\ttotal: 2m 5s\tremaining: 39m 40s\n",
      "25:\tlearn: 0.4798104\ttotal: 2m 10s\tremaining: 39m 35s\n",
      "26:\tlearn: 0.4739031\ttotal: 2m 15s\tremaining: 39m 29s\n",
      "27:\tlearn: 0.4684070\ttotal: 2m 20s\tremaining: 39m 22s\n",
      "28:\tlearn: 0.4628405\ttotal: 2m 24s\tremaining: 39m 14s\n",
      "29:\tlearn: 0.4574468\ttotal: 2m 30s\tremaining: 39m 15s\n",
      "30:\tlearn: 0.4520246\ttotal: 2m 35s\tremaining: 39m 7s\n",
      "31:\tlearn: 0.4469723\ttotal: 2m 40s\tremaining: 39m 2s\n",
      "32:\tlearn: 0.4419875\ttotal: 2m 44s\tremaining: 38m 54s\n",
      "33:\tlearn: 0.4370328\ttotal: 2m 49s\tremaining: 38m 48s\n",
      "34:\tlearn: 0.4322758\ttotal: 2m 54s\tremaining: 38m 41s\n",
      "35:\tlearn: 0.4276375\ttotal: 2m 59s\tremaining: 38m 35s\n",
      "36:\tlearn: 0.4230269\ttotal: 3m 4s\tremaining: 38m 28s\n",
      "37:\tlearn: 0.4184231\ttotal: 3m 9s\tremaining: 38m 22s\n",
      "38:\tlearn: 0.4140046\ttotal: 3m 14s\tremaining: 38m 16s\n",
      "39:\tlearn: 0.4098037\ttotal: 3m 19s\tremaining: 38m 8s\n",
      "40:\tlearn: 0.4057256\ttotal: 3m 23s\tremaining: 38m 2s\n",
      "41:\tlearn: 0.4015953\ttotal: 3m 28s\tremaining: 37m 57s\n",
      "42:\tlearn: 0.3976625\ttotal: 3m 33s\tremaining: 37m 52s\n",
      "43:\tlearn: 0.3939789\ttotal: 3m 38s\tremaining: 37m 47s\n",
      "44:\tlearn: 0.3902996\ttotal: 3m 43s\tremaining: 37m 43s\n",
      "45:\tlearn: 0.3865311\ttotal: 3m 48s\tremaining: 37m 38s\n",
      "46:\tlearn: 0.3829496\ttotal: 3m 53s\tremaining: 37m 33s\n",
      "47:\tlearn: 0.3794330\ttotal: 3m 58s\tremaining: 37m 28s\n",
      "48:\tlearn: 0.3760589\ttotal: 4m 3s\tremaining: 37m 24s\n",
      "49:\tlearn: 0.3726985\ttotal: 4m 8s\tremaining: 37m 18s\n",
      "50:\tlearn: 0.3695018\ttotal: 4m 13s\tremaining: 37m 13s\n",
      "51:\tlearn: 0.3663852\ttotal: 4m 18s\tremaining: 37m 8s\n",
      "52:\tlearn: 0.3633143\ttotal: 4m 23s\tremaining: 37m 3s\n",
      "53:\tlearn: 0.3603550\ttotal: 4m 28s\tremaining: 36m 57s\n",
      "54:\tlearn: 0.3575313\ttotal: 4m 33s\tremaining: 36m 52s\n",
      "55:\tlearn: 0.3546199\ttotal: 4m 38s\tremaining: 36m 48s\n",
      "56:\tlearn: 0.3519077\ttotal: 4m 43s\tremaining: 36m 43s\n",
      "57:\tlearn: 0.3491722\ttotal: 4m 48s\tremaining: 36m 37s\n",
      "58:\tlearn: 0.3464350\ttotal: 4m 53s\tremaining: 36m 33s\n",
      "59:\tlearn: 0.3437526\ttotal: 4m 58s\tremaining: 36m 28s\n",
      "60:\tlearn: 0.3411481\ttotal: 5m 3s\tremaining: 36m 23s\n",
      "61:\tlearn: 0.3386317\ttotal: 5m 8s\tremaining: 36m 18s\n",
      "62:\tlearn: 0.3361588\ttotal: 5m 13s\tremaining: 36m 13s\n",
      "63:\tlearn: 0.3338330\ttotal: 5m 18s\tremaining: 36m 7s\n",
      "64:\tlearn: 0.3314976\ttotal: 5m 23s\tremaining: 36m 2s\n",
      "65:\tlearn: 0.3291609\ttotal: 5m 28s\tremaining: 35m 56s\n",
      "66:\tlearn: 0.3268968\ttotal: 5m 32s\tremaining: 35m 50s\n",
      "67:\tlearn: 0.3248186\ttotal: 5m 37s\tremaining: 35m 45s\n",
      "68:\tlearn: 0.3226870\ttotal: 5m 42s\tremaining: 35m 39s\n",
      "69:\tlearn: 0.3205785\ttotal: 5m 47s\tremaining: 35m 34s\n",
      "70:\tlearn: 0.3185038\ttotal: 5m 52s\tremaining: 35m 28s\n",
      "71:\tlearn: 0.3165180\ttotal: 5m 57s\tremaining: 35m 22s\n",
      "72:\tlearn: 0.3145684\ttotal: 6m 2s\tremaining: 35m 18s\n",
      "73:\tlearn: 0.3126793\ttotal: 6m 6s\tremaining: 35m 12s\n",
      "74:\tlearn: 0.3108419\ttotal: 6m 11s\tremaining: 35m 6s\n",
      "75:\tlearn: 0.3090102\ttotal: 6m 16s\tremaining: 35m 1s\n",
      "76:\tlearn: 0.3072473\ttotal: 6m 21s\tremaining: 34m 56s\n",
      "77:\tlearn: 0.3055278\ttotal: 6m 26s\tremaining: 34m 51s\n",
      "78:\tlearn: 0.3038441\ttotal: 6m 31s\tremaining: 34m 45s\n",
      "79:\tlearn: 0.3021568\ttotal: 6m 36s\tremaining: 34m 41s\n",
      "80:\tlearn: 0.3005081\ttotal: 6m 41s\tremaining: 34m 35s\n",
      "81:\tlearn: 0.2989180\ttotal: 6m 46s\tremaining: 34m 30s\n",
      "82:\tlearn: 0.2973155\ttotal: 6m 51s\tremaining: 34m 25s\n",
      "83:\tlearn: 0.2958250\ttotal: 6m 56s\tremaining: 34m 20s\n",
      "84:\tlearn: 0.2943623\ttotal: 7m\tremaining: 34m 15s\n",
      "85:\tlearn: 0.2929194\ttotal: 7m 5s\tremaining: 34m 10s\n",
      "86:\tlearn: 0.2914971\ttotal: 7m 10s\tremaining: 34m 4s\n",
      "87:\tlearn: 0.2901141\ttotal: 7m 15s\tremaining: 33m 59s\n",
      "88:\tlearn: 0.2886836\ttotal: 7m 20s\tremaining: 33m 54s\n",
      "89:\tlearn: 0.2873741\ttotal: 7m 25s\tremaining: 33m 49s\n",
      "90:\tlearn: 0.2861062\ttotal: 7m 30s\tremaining: 33m 45s\n",
      "91:\tlearn: 0.2848319\ttotal: 7m 35s\tremaining: 33m 39s\n",
      "92:\tlearn: 0.2835657\ttotal: 7m 40s\tremaining: 33m 34s\n",
      "93:\tlearn: 0.2823472\ttotal: 7m 45s\tremaining: 33m 29s\n",
      "94:\tlearn: 0.2811624\ttotal: 7m 50s\tremaining: 33m 24s\n",
      "95:\tlearn: 0.2800221\ttotal: 7m 55s\tremaining: 33m 19s\n",
      "96:\tlearn: 0.2789078\ttotal: 7m 59s\tremaining: 33m 13s\n",
      "97:\tlearn: 0.2777956\ttotal: 8m 4s\tremaining: 33m 9s\n",
      "98:\tlearn: 0.2767079\ttotal: 8m 9s\tremaining: 33m 3s\n",
      "99:\tlearn: 0.2756431\ttotal: 8m 14s\tremaining: 32m 58s\n",
      "100:\tlearn: 0.2746202\ttotal: 8m 19s\tremaining: 32m 53s\n",
      "101:\tlearn: 0.2734397\ttotal: 8m 24s\tremaining: 32m 49s\n",
      "102:\tlearn: 0.2724226\ttotal: 8m 29s\tremaining: 32m 43s\n",
      "103:\tlearn: 0.2713967\ttotal: 8m 34s\tremaining: 32m 38s\n",
      "104:\tlearn: 0.2704524\ttotal: 8m 39s\tremaining: 32m 33s\n",
      "105:\tlearn: 0.2693553\ttotal: 8m 44s\tremaining: 32m 28s\n",
      "106:\tlearn: 0.2684539\ttotal: 8m 49s\tremaining: 32m 23s\n",
      "107:\tlearn: 0.2674056\ttotal: 8m 54s\tremaining: 32m 19s\n",
      "108:\tlearn: 0.2665388\ttotal: 8m 59s\tremaining: 32m 14s\n",
      "109:\tlearn: 0.2656662\ttotal: 9m 4s\tremaining: 32m 8s\n",
      "110:\tlearn: 0.2648029\ttotal: 9m 8s\tremaining: 32m 3s\n",
      "111:\tlearn: 0.2640109\ttotal: 9m 13s\tremaining: 31m 58s\n",
      "112:\tlearn: 0.2631705\ttotal: 9m 18s\tremaining: 31m 53s\n",
      "113:\tlearn: 0.2623568\ttotal: 9m 23s\tremaining: 31m 47s\n",
      "114:\tlearn: 0.2615867\ttotal: 9m 28s\tremaining: 31m 42s\n",
      "115:\tlearn: 0.2608116\ttotal: 9m 33s\tremaining: 31m 37s\n",
      "116:\tlearn: 0.2600424\ttotal: 9m 38s\tremaining: 31m 32s\n",
      "117:\tlearn: 0.2592928\ttotal: 9m 43s\tremaining: 31m 27s\n",
      "118:\tlearn: 0.2585716\ttotal: 9m 48s\tremaining: 31m 22s\n",
      "119:\tlearn: 0.2578464\ttotal: 9m 52s\tremaining: 31m 17s\n",
      "120:\tlearn: 0.2571332\ttotal: 9m 57s\tremaining: 31m 12s\n",
      "121:\tlearn: 0.2564517\ttotal: 10m 2s\tremaining: 31m 7s\n",
      "122:\tlearn: 0.2557590\ttotal: 10m 7s\tremaining: 31m 2s\n",
      "123:\tlearn: 0.2550898\ttotal: 10m 12s\tremaining: 30m 57s\n",
      "124:\tlearn: 0.2544337\ttotal: 10m 17s\tremaining: 30m 52s\n",
      "125:\tlearn: 0.2537677\ttotal: 10m 22s\tremaining: 30m 47s\n",
      "126:\tlearn: 0.2531379\ttotal: 10m 27s\tremaining: 30m 42s\n",
      "127:\tlearn: 0.2524022\ttotal: 10m 32s\tremaining: 30m 37s\n",
      "128:\tlearn: 0.2517992\ttotal: 10m 37s\tremaining: 30m 32s\n",
      "129:\tlearn: 0.2512055\ttotal: 10m 41s\tremaining: 30m 27s\n",
      "130:\tlearn: 0.2506042\ttotal: 10m 46s\tremaining: 30m 22s\n",
      "131:\tlearn: 0.2499374\ttotal: 10m 51s\tremaining: 30m 17s\n",
      "132:\tlearn: 0.2493525\ttotal: 10m 56s\tremaining: 30m 12s\n",
      "133:\tlearn: 0.2486632\ttotal: 11m 1s\tremaining: 30m 8s\n",
      "134:\tlearn: 0.2481259\ttotal: 11m 6s\tremaining: 30m 3s\n",
      "135:\tlearn: 0.2475849\ttotal: 11m 11s\tremaining: 29m 57s\n",
      "136:\tlearn: 0.2470549\ttotal: 11m 16s\tremaining: 29m 52s\n",
      "137:\tlearn: 0.2465385\ttotal: 11m 21s\tremaining: 29m 47s\n",
      "138:\tlearn: 0.2460267\ttotal: 11m 26s\tremaining: 29m 42s\n",
      "139:\tlearn: 0.2455218\ttotal: 11m 31s\tremaining: 29m 37s\n",
      "140:\tlearn: 0.2450084\ttotal: 11m 36s\tremaining: 29m 33s\n",
      "141:\tlearn: 0.2445102\ttotal: 11m 41s\tremaining: 29m 27s\n",
      "142:\tlearn: 0.2440448\ttotal: 11m 46s\tremaining: 29m 22s\n",
      "143:\tlearn: 0.2435612\ttotal: 11m 50s\tremaining: 29m 17s\n",
      "144:\tlearn: 0.2430968\ttotal: 11m 55s\tremaining: 29m 12s\n",
      "145:\tlearn: 0.2425286\ttotal: 12m\tremaining: 29m 8s\n",
      "146:\tlearn: 0.2420691\ttotal: 12m 5s\tremaining: 29m 3s\n",
      "147:\tlearn: 0.2415279\ttotal: 12m 10s\tremaining: 28m 58s\n",
      "148:\tlearn: 0.2410715\ttotal: 12m 15s\tremaining: 28m 53s\n",
      "149:\tlearn: 0.2406494\ttotal: 12m 20s\tremaining: 28m 48s\n",
      "150:\tlearn: 0.2402235\ttotal: 12m 25s\tremaining: 28m 43s\n",
      "151:\tlearn: 0.2398107\ttotal: 12m 30s\tremaining: 28m 38s\n",
      "152:\tlearn: 0.2393161\ttotal: 12m 35s\tremaining: 28m 33s\n",
      "153:\tlearn: 0.2389005\ttotal: 12m 40s\tremaining: 28m 28s\n",
      "154:\tlearn: 0.2384865\ttotal: 12m 45s\tremaining: 28m 23s\n",
      "155:\tlearn: 0.2380922\ttotal: 12m 50s\tremaining: 28m 18s\n",
      "156:\tlearn: 0.2376864\ttotal: 12m 55s\tremaining: 28m 13s\n",
      "157:\tlearn: 0.2372976\ttotal: 12m 59s\tremaining: 28m 8s\n",
      "158:\tlearn: 0.2369123\ttotal: 13m 4s\tremaining: 28m 3s\n",
      "159:\tlearn: 0.2364710\ttotal: 13m 9s\tremaining: 27m 58s\n",
      "160:\tlearn: 0.2360305\ttotal: 13m 14s\tremaining: 27m 53s\n",
      "161:\tlearn: 0.2356653\ttotal: 13m 19s\tremaining: 27m 48s\n",
      "162:\tlearn: 0.2353056\ttotal: 13m 24s\tremaining: 27m 43s\n",
      "163:\tlearn: 0.2349607\ttotal: 13m 29s\tremaining: 27m 38s\n",
      "164:\tlearn: 0.2346200\ttotal: 13m 34s\tremaining: 27m 33s\n",
      "165:\tlearn: 0.2342996\ttotal: 13m 39s\tremaining: 27m 28s\n",
      "166:\tlearn: 0.2339807\ttotal: 13m 44s\tremaining: 27m 23s\n",
      "167:\tlearn: 0.2336656\ttotal: 13m 49s\tremaining: 27m 18s\n",
      "168:\tlearn: 0.2333225\ttotal: 13m 54s\tremaining: 27m 13s\n",
      "169:\tlearn: 0.2330057\ttotal: 13m 58s\tremaining: 27m 8s\n",
      "170:\tlearn: 0.2326888\ttotal: 14m 3s\tremaining: 27m 3s\n",
      "171:\tlearn: 0.2323839\ttotal: 14m 8s\tremaining: 26m 58s\n",
      "172:\tlearn: 0.2320741\ttotal: 14m 13s\tremaining: 26m 53s\n",
      "173:\tlearn: 0.2317514\ttotal: 14m 18s\tremaining: 26m 48s\n",
      "174:\tlearn: 0.2314443\ttotal: 14m 23s\tremaining: 26m 43s\n",
      "175:\tlearn: 0.2311371\ttotal: 14m 28s\tremaining: 26m 38s\n",
      "176:\tlearn: 0.2308546\ttotal: 14m 33s\tremaining: 26m 33s\n",
      "177:\tlearn: 0.2304835\ttotal: 14m 38s\tremaining: 26m 29s\n",
      "178:\tlearn: 0.2301800\ttotal: 14m 43s\tremaining: 26m 23s\n",
      "179:\tlearn: 0.2298893\ttotal: 14m 48s\tremaining: 26m 18s\n",
      "180:\tlearn: 0.2295900\ttotal: 14m 53s\tremaining: 26m 13s\n",
      "181:\tlearn: 0.2293097\ttotal: 14m 57s\tremaining: 26m 8s\n",
      "182:\tlearn: 0.2290476\ttotal: 15m 2s\tremaining: 26m 4s\n",
      "183:\tlearn: 0.2287658\ttotal: 15m 7s\tremaining: 25m 59s\n",
      "184:\tlearn: 0.2284957\ttotal: 15m 12s\tremaining: 25m 54s\n",
      "185:\tlearn: 0.2282294\ttotal: 15m 17s\tremaining: 25m 49s\n",
      "186:\tlearn: 0.2279725\ttotal: 15m 22s\tremaining: 25m 44s\n",
      "187:\tlearn: 0.2276433\ttotal: 15m 27s\tremaining: 25m 39s\n",
      "188:\tlearn: 0.2273976\ttotal: 15m 32s\tremaining: 25m 34s\n",
      "189:\tlearn: 0.2271381\ttotal: 15m 37s\tremaining: 25m 29s\n",
      "190:\tlearn: 0.2268751\ttotal: 15m 42s\tremaining: 25m 24s\n",
      "191:\tlearn: 0.2265521\ttotal: 15m 47s\tremaining: 25m 19s\n",
      "192:\tlearn: 0.2263051\ttotal: 15m 52s\tremaining: 25m 14s\n",
      "193:\tlearn: 0.2260735\ttotal: 15m 57s\tremaining: 25m 10s\n",
      "194:\tlearn: 0.2257671\ttotal: 16m 2s\tremaining: 25m 5s\n",
      "195:\tlearn: 0.2254736\ttotal: 16m 7s\tremaining: 25m 1s\n",
      "196:\tlearn: 0.2252278\ttotal: 16m 12s\tremaining: 24m 56s\n",
      "197:\tlearn: 0.2249831\ttotal: 16m 17s\tremaining: 24m 51s\n",
      "198:\tlearn: 0.2247539\ttotal: 16m 22s\tremaining: 24m 46s\n",
      "199:\tlearn: 0.2245160\ttotal: 16m 27s\tremaining: 24m 41s\n",
      "200:\tlearn: 0.2243074\ttotal: 16m 32s\tremaining: 24m 36s\n",
      "201:\tlearn: 0.2240780\ttotal: 16m 37s\tremaining: 24m 31s\n",
      "202:\tlearn: 0.2238669\ttotal: 16m 42s\tremaining: 24m 26s\n",
      "203:\tlearn: 0.2236305\ttotal: 16m 47s\tremaining: 24m 21s\n",
      "204:\tlearn: 0.2234231\ttotal: 16m 52s\tremaining: 24m 16s\n",
      "205:\tlearn: 0.2232237\ttotal: 16m 57s\tremaining: 24m 11s\n",
      "206:\tlearn: 0.2230169\ttotal: 17m 1s\tremaining: 24m 6s\n",
      "207:\tlearn: 0.2227664\ttotal: 17m 7s\tremaining: 24m 1s\n",
      "208:\tlearn: 0.2225499\ttotal: 17m 11s\tremaining: 23m 56s\n",
      "209:\tlearn: 0.2223497\ttotal: 17m 16s\tremaining: 23m 51s\n",
      "210:\tlearn: 0.2221607\ttotal: 17m 21s\tremaining: 23m 46s\n",
      "211:\tlearn: 0.2219549\ttotal: 17m 26s\tremaining: 23m 42s\n",
      "212:\tlearn: 0.2217667\ttotal: 17m 31s\tremaining: 23m 37s\n",
      "213:\tlearn: 0.2215623\ttotal: 17m 36s\tremaining: 23m 32s\n",
      "214:\tlearn: 0.2213641\ttotal: 17m 41s\tremaining: 23m 27s\n",
      "215:\tlearn: 0.2211623\ttotal: 17m 46s\tremaining: 23m 22s\n",
      "216:\tlearn: 0.2209795\ttotal: 17m 51s\tremaining: 23m 17s\n",
      "217:\tlearn: 0.2207240\ttotal: 17m 56s\tremaining: 23m 12s\n",
      "218:\tlearn: 0.2205319\ttotal: 18m 1s\tremaining: 23m 8s\n",
      "219:\tlearn: 0.2203354\ttotal: 18m 6s\tremaining: 23m 3s\n",
      "220:\tlearn: 0.2201616\ttotal: 18m 11s\tremaining: 22m 58s\n",
      "221:\tlearn: 0.2199755\ttotal: 18m 16s\tremaining: 22m 53s\n",
      "222:\tlearn: 0.2197807\ttotal: 18m 21s\tremaining: 22m 48s\n",
      "223:\tlearn: 0.2196178\ttotal: 18m 26s\tremaining: 22m 43s\n",
      "224:\tlearn: 0.2194457\ttotal: 18m 31s\tremaining: 22m 38s\n",
      "225:\tlearn: 0.2192313\ttotal: 18m 36s\tremaining: 22m 33s\n",
      "226:\tlearn: 0.2190068\ttotal: 18m 41s\tremaining: 22m 28s\n",
      "227:\tlearn: 0.2188318\ttotal: 18m 46s\tremaining: 22m 23s\n",
      "228:\tlearn: 0.2186708\ttotal: 18m 51s\tremaining: 22m 18s\n",
      "229:\tlearn: 0.2185063\ttotal: 18m 56s\tremaining: 22m 13s\n",
      "230:\tlearn: 0.2183351\ttotal: 19m 1s\tremaining: 22m 8s\n",
      "231:\tlearn: 0.2181686\ttotal: 19m 5s\tremaining: 22m 3s\n",
      "232:\tlearn: 0.2180010\ttotal: 19m 10s\tremaining: 21m 58s\n",
      "233:\tlearn: 0.2178079\ttotal: 19m 15s\tremaining: 21m 53s\n",
      "234:\tlearn: 0.2176557\ttotal: 19m 20s\tremaining: 21m 49s\n",
      "235:\tlearn: 0.2174850\ttotal: 19m 25s\tremaining: 21m 44s\n",
      "236:\tlearn: 0.2173081\ttotal: 19m 30s\tremaining: 21m 39s\n",
      "237:\tlearn: 0.2170727\ttotal: 19m 35s\tremaining: 21m 34s\n",
      "238:\tlearn: 0.2169275\ttotal: 19m 40s\tremaining: 21m 29s\n",
      "239:\tlearn: 0.2167812\ttotal: 19m 45s\tremaining: 21m 24s\n",
      "240:\tlearn: 0.2166118\ttotal: 19m 50s\tremaining: 21m 19s\n",
      "241:\tlearn: 0.2164158\ttotal: 19m 55s\tremaining: 21m 14s\n",
      "242:\tlearn: 0.2162747\ttotal: 20m\tremaining: 21m 9s\n",
      "243:\tlearn: 0.2161189\ttotal: 20m 5s\tremaining: 21m 4s\n",
      "244:\tlearn: 0.2159753\ttotal: 20m 10s\tremaining: 20m 59s\n",
      "245:\tlearn: 0.2158159\ttotal: 20m 15s\tremaining: 20m 54s\n",
      "246:\tlearn: 0.2156575\ttotal: 20m 20s\tremaining: 20m 50s\n",
      "247:\tlearn: 0.2155106\ttotal: 20m 25s\tremaining: 20m 45s\n",
      "248:\tlearn: 0.2153148\ttotal: 20m 30s\tremaining: 20m 40s\n",
      "249:\tlearn: 0.2151758\ttotal: 20m 35s\tremaining: 20m 35s\n",
      "250:\tlearn: 0.2150339\ttotal: 20m 40s\tremaining: 20m 30s\n",
      "251:\tlearn: 0.2148912\ttotal: 20m 45s\tremaining: 20m 25s\n",
      "252:\tlearn: 0.2147503\ttotal: 20m 50s\tremaining: 20m 20s\n",
      "253:\tlearn: 0.2145812\ttotal: 20m 55s\tremaining: 20m 15s\n",
      "254:\tlearn: 0.2144358\ttotal: 20m 59s\tremaining: 20m 10s\n",
      "255:\tlearn: 0.2142987\ttotal: 21m 5s\tremaining: 20m 5s\n",
      "256:\tlearn: 0.2141573\ttotal: 21m 10s\tremaining: 20m\n",
      "257:\tlearn: 0.2140252\ttotal: 21m 14s\tremaining: 19m 55s\n",
      "258:\tlearn: 0.2138771\ttotal: 21m 19s\tremaining: 19m 50s\n",
      "259:\tlearn: 0.2136954\ttotal: 21m 24s\tremaining: 19m 46s\n",
      "260:\tlearn: 0.2135703\ttotal: 21m 29s\tremaining: 19m 41s\n",
      "261:\tlearn: 0.2133850\ttotal: 21m 34s\tremaining: 19m 36s\n",
      "262:\tlearn: 0.2132419\ttotal: 21m 39s\tremaining: 19m 31s\n",
      "263:\tlearn: 0.2131097\ttotal: 21m 44s\tremaining: 19m 26s\n",
      "264:\tlearn: 0.2129837\ttotal: 21m 49s\tremaining: 19m 21s\n",
      "265:\tlearn: 0.2128545\ttotal: 21m 54s\tremaining: 19m 16s\n",
      "266:\tlearn: 0.2127362\ttotal: 21m 59s\tremaining: 19m 11s\n",
      "267:\tlearn: 0.2125594\ttotal: 22m 4s\tremaining: 19m 6s\n",
      "268:\tlearn: 0.2124385\ttotal: 22m 9s\tremaining: 19m 1s\n",
      "269:\tlearn: 0.2122967\ttotal: 22m 14s\tremaining: 18m 56s\n",
      "270:\tlearn: 0.2121350\ttotal: 22m 19s\tremaining: 18m 51s\n",
      "271:\tlearn: 0.2119852\ttotal: 22m 24s\tremaining: 18m 46s\n",
      "272:\tlearn: 0.2118233\ttotal: 22m 29s\tremaining: 18m 42s\n",
      "273:\tlearn: 0.2116889\ttotal: 22m 34s\tremaining: 18m 37s\n",
      "274:\tlearn: 0.2115438\ttotal: 22m 39s\tremaining: 18m 32s\n",
      "275:\tlearn: 0.2114146\ttotal: 22m 44s\tremaining: 18m 27s\n",
      "276:\tlearn: 0.2113038\ttotal: 22m 49s\tremaining: 18m 22s\n",
      "277:\tlearn: 0.2111841\ttotal: 22m 54s\tremaining: 18m 17s\n",
      "278:\tlearn: 0.2110632\ttotal: 22m 59s\tremaining: 18m 12s\n",
      "279:\tlearn: 0.2109331\ttotal: 23m 3s\tremaining: 18m 7s\n",
      "280:\tlearn: 0.2107750\ttotal: 23m 8s\tremaining: 18m 2s\n",
      "281:\tlearn: 0.2106560\ttotal: 23m 13s\tremaining: 17m 57s\n",
      "282:\tlearn: 0.2105005\ttotal: 23m 19s\tremaining: 17m 52s\n",
      "283:\tlearn: 0.2103843\ttotal: 23m 23s\tremaining: 17m 47s\n",
      "284:\tlearn: 0.2102563\ttotal: 23m 28s\tremaining: 17m 42s\n",
      "285:\tlearn: 0.2101326\ttotal: 23m 33s\tremaining: 17m 37s\n",
      "286:\tlearn: 0.2100268\ttotal: 23m 38s\tremaining: 17m 32s\n",
      "287:\tlearn: 0.2099182\ttotal: 23m 43s\tremaining: 17m 27s\n",
      "288:\tlearn: 0.2097661\ttotal: 23m 48s\tremaining: 17m 23s\n",
      "289:\tlearn: 0.2096668\ttotal: 23m 53s\tremaining: 17m 18s\n",
      "290:\tlearn: 0.2095436\ttotal: 23m 58s\tremaining: 17m 13s\n",
      "291:\tlearn: 0.2094404\ttotal: 24m 3s\tremaining: 17m 8s\n",
      "292:\tlearn: 0.2092966\ttotal: 24m 8s\tremaining: 17m 3s\n",
      "293:\tlearn: 0.2091880\ttotal: 24m 13s\tremaining: 16m 58s\n",
      "294:\tlearn: 0.2090540\ttotal: 24m 18s\tremaining: 16m 53s\n",
      "295:\tlearn: 0.2089400\ttotal: 24m 23s\tremaining: 16m 48s\n",
      "296:\tlearn: 0.2088304\ttotal: 24m 28s\tremaining: 16m 43s\n",
      "297:\tlearn: 0.2087142\ttotal: 24m 33s\tremaining: 16m 38s\n",
      "298:\tlearn: 0.2085530\ttotal: 24m 38s\tremaining: 16m 33s\n",
      "299:\tlearn: 0.2084573\ttotal: 24m 42s\tremaining: 16m 28s\n",
      "300:\tlearn: 0.2083428\ttotal: 24m 47s\tremaining: 16m 23s\n",
      "301:\tlearn: 0.2082370\ttotal: 24m 52s\tremaining: 16m 18s\n",
      "302:\tlearn: 0.2081031\ttotal: 24m 57s\tremaining: 16m 13s\n",
      "303:\tlearn: 0.2079903\ttotal: 25m 2s\tremaining: 16m 8s\n",
      "304:\tlearn: 0.2078705\ttotal: 25m 7s\tremaining: 16m 3s\n",
      "305:\tlearn: 0.2077679\ttotal: 25m 12s\tremaining: 15m 58s\n",
      "306:\tlearn: 0.2076290\ttotal: 25m 17s\tremaining: 15m 54s\n",
      "307:\tlearn: 0.2075274\ttotal: 25m 22s\tremaining: 15m 49s\n",
      "308:\tlearn: 0.2074344\ttotal: 25m 27s\tremaining: 15m 44s\n",
      "309:\tlearn: 0.2073314\ttotal: 25m 32s\tremaining: 15m 39s\n",
      "310:\tlearn: 0.2072251\ttotal: 25m 36s\tremaining: 15m 34s\n",
      "311:\tlearn: 0.2070784\ttotal: 25m 42s\tremaining: 15m 29s\n",
      "312:\tlearn: 0.2069755\ttotal: 25m 46s\tremaining: 15m 24s\n",
      "313:\tlearn: 0.2068855\ttotal: 25m 51s\tremaining: 15m 19s\n",
      "314:\tlearn: 0.2067884\ttotal: 25m 56s\tremaining: 15m 14s\n",
      "315:\tlearn: 0.2066650\ttotal: 26m 1s\tremaining: 15m 9s\n",
      "316:\tlearn: 0.2065390\ttotal: 26m 6s\tremaining: 15m 4s\n",
      "317:\tlearn: 0.2064149\ttotal: 26m 11s\tremaining: 14m 59s\n",
      "318:\tlearn: 0.2063017\ttotal: 26m 16s\tremaining: 14m 54s\n",
      "319:\tlearn: 0.2061624\ttotal: 26m 21s\tremaining: 14m 49s\n",
      "320:\tlearn: 0.2060568\ttotal: 26m 26s\tremaining: 14m 44s\n",
      "321:\tlearn: 0.2059108\ttotal: 26m 31s\tremaining: 14m 39s\n",
      "322:\tlearn: 0.2058141\ttotal: 26m 36s\tremaining: 14m 34s\n",
      "323:\tlearn: 0.2057140\ttotal: 26m 41s\tremaining: 14m 29s\n",
      "324:\tlearn: 0.2056030\ttotal: 26m 46s\tremaining: 14m 24s\n",
      "325:\tlearn: 0.2054558\ttotal: 26m 51s\tremaining: 14m 20s\n",
      "326:\tlearn: 0.2053690\ttotal: 26m 56s\tremaining: 14m 15s\n",
      "327:\tlearn: 0.2052816\ttotal: 27m 1s\tremaining: 14m 10s\n",
      "328:\tlearn: 0.2051648\ttotal: 27m 6s\tremaining: 14m 5s\n",
      "329:\tlearn: 0.2050596\ttotal: 27m 10s\tremaining: 14m\n",
      "330:\tlearn: 0.2049598\ttotal: 27m 15s\tremaining: 13m 55s\n",
      "331:\tlearn: 0.2048651\ttotal: 27m 20s\tremaining: 13m 50s\n",
      "332:\tlearn: 0.2047726\ttotal: 27m 25s\tremaining: 13m 45s\n",
      "333:\tlearn: 0.2046664\ttotal: 27m 30s\tremaining: 13m 40s\n",
      "334:\tlearn: 0.2045712\ttotal: 27m 35s\tremaining: 13m 35s\n",
      "335:\tlearn: 0.2044705\ttotal: 27m 40s\tremaining: 13m 30s\n",
      "336:\tlearn: 0.2043831\ttotal: 27m 45s\tremaining: 13m 25s\n",
      "337:\tlearn: 0.2042756\ttotal: 27m 50s\tremaining: 13m 20s\n",
      "338:\tlearn: 0.2041643\ttotal: 27m 55s\tremaining: 13m 15s\n",
      "339:\tlearn: 0.2040742\ttotal: 28m\tremaining: 13m 10s\n",
      "340:\tlearn: 0.2039786\ttotal: 28m 5s\tremaining: 13m 5s\n",
      "341:\tlearn: 0.2038902\ttotal: 28m 10s\tremaining: 13m\n",
      "342:\tlearn: 0.2037568\ttotal: 28m 15s\tremaining: 12m 56s\n",
      "343:\tlearn: 0.2036540\ttotal: 28m 20s\tremaining: 12m 51s\n",
      "344:\tlearn: 0.2035684\ttotal: 28m 25s\tremaining: 12m 46s\n",
      "345:\tlearn: 0.2034610\ttotal: 28m 30s\tremaining: 12m 41s\n",
      "346:\tlearn: 0.2033490\ttotal: 28m 35s\tremaining: 12m 36s\n",
      "347:\tlearn: 0.2032554\ttotal: 28m 40s\tremaining: 12m 31s\n",
      "348:\tlearn: 0.2031633\ttotal: 28m 45s\tremaining: 12m 26s\n",
      "349:\tlearn: 0.2030590\ttotal: 28m 50s\tremaining: 12m 21s\n",
      "350:\tlearn: 0.2029640\ttotal: 28m 55s\tremaining: 12m 16s\n",
      "351:\tlearn: 0.2028675\ttotal: 29m\tremaining: 12m 11s\n",
      "352:\tlearn: 0.2027334\ttotal: 29m 5s\tremaining: 12m 6s\n",
      "353:\tlearn: 0.2026376\ttotal: 29m 10s\tremaining: 12m 1s\n",
      "354:\tlearn: 0.2025420\ttotal: 29m 15s\tremaining: 11m 57s\n",
      "355:\tlearn: 0.2024348\ttotal: 29m 20s\tremaining: 11m 52s\n",
      "356:\tlearn: 0.2023461\ttotal: 29m 25s\tremaining: 11m 47s\n",
      "357:\tlearn: 0.2022195\ttotal: 29m 30s\tremaining: 11m 42s\n",
      "358:\tlearn: 0.2021253\ttotal: 29m 35s\tremaining: 11m 37s\n",
      "359:\tlearn: 0.2020107\ttotal: 29m 40s\tremaining: 11m 32s\n",
      "360:\tlearn: 0.2019200\ttotal: 29m 45s\tremaining: 11m 27s\n",
      "361:\tlearn: 0.2018083\ttotal: 29m 50s\tremaining: 11m 22s\n",
      "362:\tlearn: 0.2017125\ttotal: 29m 54s\tremaining: 11m 17s\n",
      "363:\tlearn: 0.2016361\ttotal: 29m 59s\tremaining: 11m 12s\n",
      "364:\tlearn: 0.2015568\ttotal: 30m 4s\tremaining: 11m 7s\n",
      "365:\tlearn: 0.2014794\ttotal: 30m 9s\tremaining: 11m 2s\n",
      "366:\tlearn: 0.2014004\ttotal: 30m 14s\tremaining: 10m 57s\n",
      "367:\tlearn: 0.2013220\ttotal: 30m 19s\tremaining: 10m 52s\n",
      "368:\tlearn: 0.2012359\ttotal: 30m 24s\tremaining: 10m 47s\n",
      "369:\tlearn: 0.2011588\ttotal: 30m 29s\tremaining: 10m 42s\n",
      "370:\tlearn: 0.2010473\ttotal: 30m 34s\tremaining: 10m 37s\n",
      "371:\tlearn: 0.2009682\ttotal: 30m 39s\tremaining: 10m 32s\n",
      "372:\tlearn: 0.2008635\ttotal: 30m 44s\tremaining: 10m 27s\n",
      "373:\tlearn: 0.2007756\ttotal: 30m 49s\tremaining: 10m 22s\n",
      "374:\tlearn: 0.2006565\ttotal: 30m 54s\tremaining: 10m 18s\n",
      "375:\tlearn: 0.2005791\ttotal: 30m 59s\tremaining: 10m 13s\n",
      "376:\tlearn: 0.2004755\ttotal: 31m 4s\tremaining: 10m 8s\n",
      "377:\tlearn: 0.2003838\ttotal: 31m 9s\tremaining: 10m 3s\n",
      "378:\tlearn: 0.2002950\ttotal: 31m 14s\tremaining: 9m 58s\n",
      "379:\tlearn: 0.2002080\ttotal: 31m 18s\tremaining: 9m 53s\n",
      "380:\tlearn: 0.2000905\ttotal: 31m 23s\tremaining: 9m 48s\n",
      "381:\tlearn: 0.2000141\ttotal: 31m 28s\tremaining: 9m 43s\n",
      "382:\tlearn: 0.1999157\ttotal: 31m 33s\tremaining: 9m 38s\n",
      "383:\tlearn: 0.1998142\ttotal: 31m 38s\tremaining: 9m 33s\n",
      "384:\tlearn: 0.1997424\ttotal: 31m 43s\tremaining: 9m 28s\n",
      "385:\tlearn: 0.1996516\ttotal: 31m 48s\tremaining: 9m 23s\n",
      "386:\tlearn: 0.1995499\ttotal: 31m 53s\tremaining: 9m 18s\n",
      "387:\tlearn: 0.1994403\ttotal: 31m 58s\tremaining: 9m 13s\n",
      "388:\tlearn: 0.1993544\ttotal: 32m 3s\tremaining: 9m 8s\n",
      "389:\tlearn: 0.1992737\ttotal: 32m 8s\tremaining: 9m 3s\n",
      "390:\tlearn: 0.1992055\ttotal: 32m 13s\tremaining: 8m 58s\n",
      "391:\tlearn: 0.1991305\ttotal: 32m 17s\tremaining: 8m 53s\n",
      "392:\tlearn: 0.1990404\ttotal: 32m 22s\tremaining: 8m 48s\n",
      "393:\tlearn: 0.1989740\ttotal: 32m 27s\tremaining: 8m 44s\n",
      "394:\tlearn: 0.1989005\ttotal: 32m 32s\tremaining: 8m 39s\n",
      "395:\tlearn: 0.1988116\ttotal: 32m 37s\tremaining: 8m 34s\n",
      "396:\tlearn: 0.1987464\ttotal: 32m 42s\tremaining: 8m 29s\n",
      "397:\tlearn: 0.1986715\ttotal: 32m 47s\tremaining: 8m 24s\n",
      "398:\tlearn: 0.1985942\ttotal: 32m 52s\tremaining: 8m 19s\n",
      "399:\tlearn: 0.1984926\ttotal: 32m 57s\tremaining: 8m 14s\n",
      "400:\tlearn: 0.1983812\ttotal: 33m 1s\tremaining: 8m 9s\n",
      "401:\tlearn: 0.1982956\ttotal: 33m 6s\tremaining: 8m 4s\n",
      "402:\tlearn: 0.1982161\ttotal: 33m 11s\tremaining: 7m 59s\n",
      "403:\tlearn: 0.1981186\ttotal: 33m 16s\tremaining: 7m 54s\n",
      "404:\tlearn: 0.1980335\ttotal: 33m 21s\tremaining: 7m 49s\n",
      "405:\tlearn: 0.1979404\ttotal: 33m 26s\tremaining: 7m 44s\n",
      "406:\tlearn: 0.1978763\ttotal: 33m 31s\tremaining: 7m 39s\n",
      "407:\tlearn: 0.1978075\ttotal: 33m 36s\tremaining: 7m 34s\n",
      "408:\tlearn: 0.1977177\ttotal: 33m 41s\tremaining: 7m 29s\n",
      "409:\tlearn: 0.1976241\ttotal: 33m 46s\tremaining: 7m 24s\n",
      "410:\tlearn: 0.1975501\ttotal: 33m 51s\tremaining: 7m 19s\n",
      "411:\tlearn: 0.1974818\ttotal: 33m 56s\tremaining: 7m 14s\n",
      "412:\tlearn: 0.1974171\ttotal: 34m 1s\tremaining: 7m 9s\n",
      "413:\tlearn: 0.1973476\ttotal: 34m 6s\tremaining: 7m 5s\n",
      "414:\tlearn: 0.1972875\ttotal: 34m 10s\tremaining: 7m\n",
      "415:\tlearn: 0.1972240\ttotal: 34m 15s\tremaining: 6m 55s\n",
      "416:\tlearn: 0.1971372\ttotal: 34m 20s\tremaining: 6m 50s\n",
      "417:\tlearn: 0.1970657\ttotal: 34m 25s\tremaining: 6m 45s\n",
      "418:\tlearn: 0.1969879\ttotal: 34m 30s\tremaining: 6m 40s\n",
      "419:\tlearn: 0.1968722\ttotal: 34m 35s\tremaining: 6m 35s\n",
      "420:\tlearn: 0.1968063\ttotal: 34m 40s\tremaining: 6m 30s\n",
      "421:\tlearn: 0.1966998\ttotal: 34m 45s\tremaining: 6m 25s\n",
      "422:\tlearn: 0.1966249\ttotal: 34m 50s\tremaining: 6m 20s\n",
      "423:\tlearn: 0.1965524\ttotal: 34m 55s\tremaining: 6m 15s\n",
      "424:\tlearn: 0.1964786\ttotal: 35m\tremaining: 6m 10s\n",
      "425:\tlearn: 0.1964135\ttotal: 35m 5s\tremaining: 6m 5s\n",
      "426:\tlearn: 0.1963379\ttotal: 35m 10s\tremaining: 6m\n",
      "427:\tlearn: 0.1962704\ttotal: 35m 15s\tremaining: 5m 55s\n",
      "428:\tlearn: 0.1961835\ttotal: 35m 20s\tremaining: 5m 50s\n",
      "429:\tlearn: 0.1961196\ttotal: 35m 25s\tremaining: 5m 46s\n",
      "430:\tlearn: 0.1960318\ttotal: 35m 30s\tremaining: 5m 41s\n",
      "431:\tlearn: 0.1959506\ttotal: 35m 35s\tremaining: 5m 36s\n",
      "432:\tlearn: 0.1958712\ttotal: 35m 40s\tremaining: 5m 31s\n",
      "433:\tlearn: 0.1957836\ttotal: 35m 45s\tremaining: 5m 26s\n",
      "434:\tlearn: 0.1956909\ttotal: 35m 50s\tremaining: 5m 21s\n",
      "435:\tlearn: 0.1956174\ttotal: 35m 55s\tremaining: 5m 16s\n",
      "436:\tlearn: 0.1955403\ttotal: 36m\tremaining: 5m 11s\n",
      "437:\tlearn: 0.1954701\ttotal: 36m 5s\tremaining: 5m 6s\n",
      "438:\tlearn: 0.1953991\ttotal: 36m 10s\tremaining: 5m 1s\n",
      "439:\tlearn: 0.1953061\ttotal: 36m 15s\tremaining: 4m 56s\n",
      "440:\tlearn: 0.1952217\ttotal: 36m 20s\tremaining: 4m 51s\n",
      "441:\tlearn: 0.1951622\ttotal: 36m 25s\tremaining: 4m 46s\n",
      "442:\tlearn: 0.1950884\ttotal: 36m 30s\tremaining: 4m 41s\n",
      "443:\tlearn: 0.1950225\ttotal: 36m 35s\tremaining: 4m 36s\n",
      "444:\tlearn: 0.1949436\ttotal: 36m 40s\tremaining: 4m 32s\n",
      "445:\tlearn: 0.1948620\ttotal: 36m 45s\tremaining: 4m 27s\n",
      "446:\tlearn: 0.1947607\ttotal: 36m 50s\tremaining: 4m 22s\n",
      "447:\tlearn: 0.1947040\ttotal: 36m 55s\tremaining: 4m 17s\n",
      "448:\tlearn: 0.1946283\ttotal: 37m\tremaining: 4m 12s\n",
      "449:\tlearn: 0.1945658\ttotal: 37m 5s\tremaining: 4m 7s\n",
      "450:\tlearn: 0.1944645\ttotal: 37m 11s\tremaining: 4m 2s\n",
      "451:\tlearn: 0.1943903\ttotal: 37m 15s\tremaining: 3m 57s\n",
      "452:\tlearn: 0.1943159\ttotal: 37m 21s\tremaining: 3m 52s\n",
      "453:\tlearn: 0.1942508\ttotal: 37m 25s\tremaining: 3m 47s\n",
      "454:\tlearn: 0.1941689\ttotal: 37m 31s\tremaining: 3m 42s\n",
      "455:\tlearn: 0.1941050\ttotal: 37m 35s\tremaining: 3m 37s\n",
      "456:\tlearn: 0.1940512\ttotal: 37m 40s\tremaining: 3m 32s\n",
      "457:\tlearn: 0.1939672\ttotal: 37m 45s\tremaining: 3m 27s\n",
      "458:\tlearn: 0.1939118\ttotal: 37m 50s\tremaining: 3m 22s\n",
      "459:\tlearn: 0.1938532\ttotal: 37m 55s\tremaining: 3m 17s\n",
      "460:\tlearn: 0.1937979\ttotal: 38m\tremaining: 3m 12s\n",
      "461:\tlearn: 0.1937044\ttotal: 38m 5s\tremaining: 3m 8s\n",
      "462:\tlearn: 0.1936433\ttotal: 38m 10s\tremaining: 3m 3s\n",
      "463:\tlearn: 0.1935732\ttotal: 38m 15s\tremaining: 2m 58s\n",
      "464:\tlearn: 0.1934914\ttotal: 38m 20s\tremaining: 2m 53s\n",
      "465:\tlearn: 0.1934234\ttotal: 38m 25s\tremaining: 2m 48s\n",
      "466:\tlearn: 0.1933584\ttotal: 38m 30s\tremaining: 2m 43s\n",
      "467:\tlearn: 0.1932907\ttotal: 38m 35s\tremaining: 2m 38s\n",
      "468:\tlearn: 0.1932294\ttotal: 38m 40s\tremaining: 2m 33s\n",
      "469:\tlearn: 0.1931536\ttotal: 38m 45s\tremaining: 2m 28s\n",
      "470:\tlearn: 0.1930800\ttotal: 38m 50s\tremaining: 2m 23s\n",
      "471:\tlearn: 0.1930076\ttotal: 38m 55s\tremaining: 2m 18s\n",
      "472:\tlearn: 0.1929282\ttotal: 39m\tremaining: 2m 13s\n",
      "473:\tlearn: 0.1928457\ttotal: 39m 5s\tremaining: 2m 8s\n",
      "474:\tlearn: 0.1927701\ttotal: 39m 10s\tremaining: 2m 3s\n",
      "475:\tlearn: 0.1926764\ttotal: 39m 15s\tremaining: 1m 58s\n",
      "476:\tlearn: 0.1925841\ttotal: 39m 20s\tremaining: 1m 53s\n",
      "477:\tlearn: 0.1925020\ttotal: 39m 25s\tremaining: 1m 48s\n",
      "478:\tlearn: 0.1924306\ttotal: 39m 30s\tremaining: 1m 43s\n",
      "479:\tlearn: 0.1923722\ttotal: 39m 34s\tremaining: 1m 38s\n",
      "480:\tlearn: 0.1923191\ttotal: 39m 39s\tremaining: 1m 34s\n",
      "481:\tlearn: 0.1922584\ttotal: 39m 44s\tremaining: 1m 29s\n",
      "482:\tlearn: 0.1921874\ttotal: 39m 49s\tremaining: 1m 24s\n",
      "483:\tlearn: 0.1921170\ttotal: 39m 54s\tremaining: 1m 19s\n",
      "484:\tlearn: 0.1920651\ttotal: 39m 59s\tremaining: 1m 14s\n",
      "485:\tlearn: 0.1919770\ttotal: 40m 4s\tremaining: 1m 9s\n",
      "486:\tlearn: 0.1918962\ttotal: 40m 9s\tremaining: 1m 4s\n",
      "487:\tlearn: 0.1918358\ttotal: 40m 14s\tremaining: 59.4s\n",
      "488:\tlearn: 0.1917790\ttotal: 40m 19s\tremaining: 54.4s\n",
      "489:\tlearn: 0.1917270\ttotal: 40m 24s\tremaining: 49.5s\n",
      "490:\tlearn: 0.1916546\ttotal: 40m 29s\tremaining: 44.5s\n",
      "491:\tlearn: 0.1915987\ttotal: 40m 34s\tremaining: 39.6s\n",
      "492:\tlearn: 0.1915447\ttotal: 40m 38s\tremaining: 34.6s\n",
      "493:\tlearn: 0.1914915\ttotal: 40m 43s\tremaining: 29.7s\n",
      "494:\tlearn: 0.1914101\ttotal: 40m 48s\tremaining: 24.7s\n",
      "495:\tlearn: 0.1913198\ttotal: 40m 53s\tremaining: 19.8s\n",
      "496:\tlearn: 0.1912580\ttotal: 40m 58s\tremaining: 14.8s\n",
      "497:\tlearn: 0.1911834\ttotal: 41m 3s\tremaining: 9.89s\n",
      "498:\tlearn: 0.1911278\ttotal: 41m 8s\tremaining: 4.95s\n",
      "499:\tlearn: 0.1910371\ttotal: 41m 13s\tremaining: 0us\n",
      "Размер матрицы: (31915,)\n",
      "Размер матрицы: (31915,)\n",
      "f1 cb: 0.5958638517880224\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostClassifier(iterations=500, learning_rate = 0.01,loss_function='CrossEntropy',random_seed=42)\n",
    "cb.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_cb = cb.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_cb.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_cb = f1_score(target_test, predictions_cb)\n",
    "print(\"f1 cb:\",f1_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (31915,)\n",
      "Размер матрицы: (31915,)\n",
      "f1 clf: 0.5835356433458398\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear',C=0.05,random_state=12345)\n",
    "clf.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_clf = clf.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_clf.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_clf = f1_score(target_test, predictions_clf)\n",
    "print(\"f1 clf:\",f1_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (31915,)\n",
      "Размер матрицы: (31915,)\n",
      "f1 knn: 0.056303549571603426\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "knn.fit(tf_idf_train_feautures, target_train)\n",
    "predictions_knn = knn.predict(tf_idf_test_feautures)\n",
    "print(\"Matrix shape:\", predictions_knn.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_knn = f1_score(target_test, predictions_knn)\n",
    "print(\"f1 knn:\",f1_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ones = df[df['toxic'] == 1].sample(10000, random_state=123)\n",
    "df_zeros = df[df['toxic'] == 0].sample(10000, random_state=123)\n",
    "new_df = shuffle(pd.concat([df_ones] + [df_zeros]))\n",
    "\n",
    "tokenized = new_df['text'].apply(lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n",
    "max_len = max(map(len, tokenized))\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "005eff2659fa4601b75517c169d1d25b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20 \n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.cuda.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.cuda.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 train: 0.8733\nF1 test: 0.8404\n"
     ]
    }
   ],
   "source": [
    "features_bert = np.concatenate(embeddings)\n",
    "target_bert = new_df['toxic']\n",
    "features_train_bert, features_test_bert, target_train_bert, target_test_bert = train_test_split(features_bert, target_bert, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 train: 0.8733\nF1 test: 0.8404\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(solver='liblinear', random_state=123455)\n",
    "model_lr.fit(features_train_bert, target_train_bert)\n",
    "\n",
    "pred_train = model_lr.predict(features_train_bert)\n",
    "pred_test = model_lr.predict(features_test_bert)\n",
    "\n",
    "print('F1 train: {:.4f}'.format(f1_score(target_train_bert, pred_train)))\n",
    "print('F1 test: {:.4f}'.format(f1_score(target_test_bert, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SGDClassifier(alpha=0.01, max_iter=100, random_state=12345)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "modelsgd = SGDClassifier(max_iter=100,shuffle=True,random_state=12345)\n",
    "grid_search_sgd = GridSearchCV(modelsgd, param_grid=params)\n",
    "grid_search_sgd.fit(features_train_bert, target_train_bert)\n",
    "print(grid_search_sgd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix shape: (4000,)\nMatrix shape: (400,)\nf1 sgd: 0.8484401866863179\n"
     ]
    }
   ],
   "source": [
    "sgd_bert = SGDClassifier(alpha=0.01, max_iter=100, random_state=12345)\n",
    "sgd.fit(features_train_bert, target_train_bert)\n",
    "predictions_sgd = sgd.predict(features_test_bert)\n",
    "print(\"Matrix shape:\", predictions_sgd.shape)\n",
    "print(\"Matrix shape:\", target_test.shape)\n",
    "f1_sgd = f1_score(target_test_bert, predictions_sgd)\n",
    "print(\"f1 BERT SGD:\",f1_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}